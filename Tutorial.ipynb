{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import sys\n",
    "\n",
    "# read in the image\n",
    "img = cv.imread(cv.samples.findFile('starry_night.png'))\n",
    "\n",
    "if img is None:\n",
    "    sys.exit('Could not read image.')\n",
    "\n",
    "# first argumnet is the title\n",
    "cv.imshow(\"Display image\", img)\n",
    "\n",
    "# the argument specifies the time, in milliseconds, the program should wait for, to register a key press\n",
    "# 0 means wait forever\n",
    "k = cv.waitKey(0)\n",
    "\n",
    "# to write the image if the 's' key is pressed\n",
    "if k==ord('s'):\n",
    "    cv.imwrite('starry_night.png', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and Saving Video from camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# save video file\n",
    "# define the codec and create VideoWriter object\n",
    "# fourcc is a 4-byte code used to specify the video codec\n",
    "fourcc = cv.VideoWriter_fourcc(*'XVID')\n",
    "out = cv.VideoWriter('output.avi', fourcc, 20.0, (640, 480))\n",
    "\n",
    "# the argument specifies the camera number\n",
    "# to play a video file, instead of 0 write file name\n",
    "cap = cv.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # if frame is read correctly, ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame..Exiting\")\n",
    "        break\n",
    "    \n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    # Dsiplay the resulting frame\n",
    "    cv.imshow('frame',gray)\n",
    "    \n",
    "    gray = cv.flip(gray, 0)\n",
    "    # write the flipped frame\n",
    "    out.write(gray)\n",
    "    \n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# when everything is done, release resources\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing functions in OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a black image\n",
    "img = np.zeros((512, 512, 3), np.uint8)\n",
    "\n",
    "# Draw a diagonal blue line (blue in BGR is(255,0,0)) with thickness 5px\n",
    "cv.line(img, (0,0), (511, 511), (255, 0, 0), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a circle\n",
    "cv.circle(img, (447, 63), 63, (0, 0, 255), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a rectangle\n",
    "cv.rectangle(img,(384,0),(510,128),(0,255,0),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing an ellipse, giving the angle of rotation in anti-clockwise direction, starta angle, end angle etc.\n",
    "cv.ellipse(img,(256,256),(100,50),0,0,180,255,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a polygon\n",
    "# coordinates of vertices\n",
    "pts = np.array([[10,5],[20,30],[70,20],[50,10]], np.int32)\n",
    "# Converting into an array of shape ROWSx1x2 where ROWS are the number of vertices\n",
    "pts = pts.reshape((-1,1,2))\n",
    "cv.polylines(img,[pts],True,(0,255,255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding text to images\n",
    "font = cv.FONT_HERSHEY_SIMPLEX\n",
    "cv.putText(img,'OpenCV',(10,500), font, 4,(255,255,255),2,cv.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    cv.imshow('image', img)\n",
    "    if cv.waitKey(0) == ord('q'):\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Operations on Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('starry_night.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px = img[100,100]\n",
    "print(px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing only blue pixel\n",
    "blue = img[100, 100, 0]\n",
    "print(blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying pixel values\n",
    "img[100, 100] = [255, 255, 255]\n",
    "print(img[100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    cv.imshow('image', img)\n",
    "    if cv.waitKey(0) == ord('q'):\n",
    "        break\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better individual pixel accessing and editing methods\n",
    "# accessing red value\n",
    "img.item(10, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.itemset((10, 10, 2), 100)\n",
    "img.item(10, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of pixels\n",
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image ROI\n",
    "part = img[280:340,330:390]\n",
    "img[273:333, 100:160] = part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting and merging image channels\n",
    "b, g, r = cv.split(img)\n",
    "img = cv.merge((b, g, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also do like\n",
    "b = img[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to change all red pixel values to zero, using numpy indexing\n",
    "img[:,:,2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making borders for images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BLUE=[255, 0, 0]\n",
    "img1 = cv.imread('starry_night.png')\n",
    "\n",
    "# the numbers are the top, bottom, left and right border widths repsectively\n",
    "replicate = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_REPLICATE)\n",
    "reflect = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_REFLECT)\n",
    "reflect101 = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_REFLECT_101)\n",
    "wrap = cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_WRAP)\n",
    "constant= cv.copyMakeBorder(img1,10,10,10,10,cv.BORDER_CONSTANT,value=BLUE)\n",
    "plt.subplot(231),plt.imshow(img1,'gray'),plt.title('ORIGINAL')\n",
    "plt.subplot(232),plt.imshow(replicate,'gray'),plt.title('REPLICATE')\n",
    "plt.subplot(233),plt.imshow(reflect,'gray'),plt.title('REFLECT')\n",
    "plt.subplot(234),plt.imshow(reflect101,'gray'),plt.title('REFLECT_101')\n",
    "plt.subplot(235),plt.imshow(wrap,'gray'),plt.title('WRAP')\n",
    "plt.subplot(236),plt.imshow(constant,'gray'),plt.title('CONSTANT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "x = np.uint8([250])\n",
    "y = np.uint8([10])\n",
    "\n",
    "print(cv.add(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv.imread('starry_night.png')\n",
    "img2 = cv.imread('opencv_logo.png')\n",
    "\n",
    "dst = cv.addWeighted(img1, 0.7, img2, 0.3, 0)\n",
    "\n",
    "cv.imshow('dst', dst)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitwise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the images\n",
    "# Load two images\n",
    "img2 = cv.imread('starry_night.png')\n",
    "img1 = cv.imread('opencv_logo.png')\n",
    "# I want to put logo on top-left corner, So I create a ROI\n",
    "rows,cols,channels = img2.shape\n",
    "roi = img1[0:rows, 0:cols]\n",
    "\n",
    "# Now create a mask of logo and create its inverse mask also\n",
    "img2gray = cv.cvtColor(img2,cv.COLOR_BGR2GRAY)\n",
    "# is value less than threshold, then pixel value set to 10(min) otherwise set to 255(max)\n",
    "ret, mask = cv.threshold(img2gray, 10, 255, cv.THRESH_BINARY)\n",
    "mask_inv = cv.bitwise_not(mask)\n",
    "# Now black-out the area of logo in ROI\n",
    "img1_bg = cv.bitwise_and(roi,roi,mask = mask_inv)\n",
    "# Take only region of logo from logo image.\n",
    "img2_fg = cv.bitwise_and(img2,img2,mask = mask)\n",
    "# Put logo in ROI and modify the main image\n",
    "dst = cv.add(img1_bg,img2_fg)\n",
    "img1[0:rows, 0:cols ] = dst\n",
    "cv.imshow('res',img1)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    # convert bgr to hsv\n",
    "    hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "    \n",
    "    # define the ranges for the blue color in hsv\n",
    "    lower_blue = np.array([110, 50, 50])\n",
    "    upper_blue = np.array([130, 255, 255])\n",
    "    \n",
    "    # threshold the hsv image to get only blue color\n",
    "    mask = cv.inRange(hsv, lower_blue, upper_blue)\n",
    "    \n",
    "    # bitwise-and mask and original image\n",
    "    res = cv.bitwise_and(frame, frame, mask = mask)\n",
    "    \n",
    "    cv.imshow('frame', frame)\n",
    "    cv.imshow('mask', mask)\n",
    "    cv.imshow('res', res)\n",
    "    \n",
    "    k = cv.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to find hsv values to track?\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "green = np.uint8([[[0, 255, 0]]])\n",
    "hsv_green = cv.cvtColor(green, cv.COLOR_BGR2HSV)\n",
    "print(hsv_green)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric transformation on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preferable interpolation methods are cv.INTER_AREA for shrinking and cv.INTER_LINEAR & cv.INTER_CUBIC for zooming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "\n",
    "img = cv.imread('opencv_logo.png')\n",
    "res = cv.resize(img, None, fx = 2, fy = 2, interpolation = cv.INTER_CUBIC)\n",
    "\n",
    "# OR\n",
    "\n",
    "height, width = img.shape[:2]\n",
    "res = cv.resize(img, (2*width, 2*height), interpolation = cv.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translation\n",
    "\n",
    "img = cv.imread('starry_night.png')\n",
    "rows, cols = img.shape[:2]\n",
    "\n",
    "# now the translation matrix, for a shift of (100, 50)\n",
    "M = np.float32([[1, 0, 100], [0, 1, 50]])\n",
    "# the third argument of cv.warpAffine is the size of output image as (width, height), where width is number of columns\n",
    "dst = cv.warpAffine(img, M, (cols, rows))\n",
    "\n",
    "cv.imshow('dst', dst)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation\n",
    "\n",
    "img = cv.imread('starry_night.png')\n",
    "rows, cols = img.shape[:2]\n",
    "\n",
    "# cols-1 and rows-1 are the coordinate limits\n",
    "# here, rotating by 90 degrees, wrt center, without any scaling\n",
    "# the first argument is the center of the image, as it is being rotated about the center\n",
    "M = cv.getRotationMatrix2D(((cols-1)/2.0, (rows-1)/2.0), 90, 1)\n",
    "dst = cv.warpAffine(img, M, (cols, rows))\n",
    "\n",
    "cv.imshow('dst', dst)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affine transformation - lines that are parallel still remain parallel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we need three points from input image and their corresponding positions in output image\n",
    "pts1 = np.float32([[50,50],[200,50],[50,200]])\n",
    "pts2 = np.float32([[10,100],[200,50],[100,250]])\n",
    "\n",
    "# get the affine transformation matrix\n",
    "M = cv.getAffineTransform(pts1, pts2)\n",
    "\n",
    "dst = cv.warpAffine(img, M, (cols, rows))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Input')\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perspective transformation\n",
    "\n",
    "# we need 4 points on input image, and corresponding points on the output images\n",
    "# out of the 4 points provided, three should not be collinear\n",
    "pts1 = np.float32([[56,65],[368,52],[28,387],[389,390]])\n",
    "pts2 = np.float32([[0,0],[300,0],[0,300],[300,300]])\n",
    "\n",
    "# get the perspective transformation matrix\n",
    "M = cv.getPerspectiveTransform(pts1, pts2)\n",
    "\n",
    "dst = cv.warpPerspective(img, M, (cols, rows))\n",
    "\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Input')\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixel value is set to 0 value if value less than threshold, otherwise set to the max value. There are many thresholding functions. The image should be a grayscale image\n",
    "The method return two values, the first is the threshold that was used and second is the thresholded image\n",
    "\n",
    "The second argument is the threshold value and the thrid argument is the max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('starry_night.png')\n",
    "ret, thresh1 = cv.threshold(img, 127, 255, cv.THRESH_BINARY)\n",
    "ret,thresh2 = cv.threshold(img,127,255,cv.THRESH_BINARY_INV)\n",
    "ret,thresh3 = cv.threshold(img,127,255,cv.THRESH_TRUNC)\n",
    "ret,thresh4 = cv.threshold(img,127,255,cv.THRESH_TOZERO)\n",
    "ret,thresh5 = cv.threshold(img,127,255,cv.THRESH_TOZERO_INV)\n",
    "\n",
    "titles = ['Original Image','BINARY','BINARY_INV','TRUNC','TOZERO','TOZERO_INV']\n",
    "images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1),plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of thresholding is the adaptive thresholding, that is determines the threshold value based on a small region around it. It is useful where different parts have different illuminations. \n",
    "\n",
    "Block size (second-to-last argument) determines size of neighborhood and C(last argument) is the constant to be subtracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv.imread('starry_night.png', 0)\n",
    "img = cv.medianBlur(img,5)\n",
    "\n",
    "ret,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
    "th2 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_MEAN_C,\\\n",
    "            cv.THRESH_BINARY,11,2)\n",
    "th3 = cv.adaptiveThreshold(img,255,cv.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "            cv.THRESH_BINARY,11,2)\n",
    "titles = ['Original Image', 'Global Thresholding (v = 127)',\n",
    "            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\n",
    "images = [img, th1, th2, th3]\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1),plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last type of thresholding is Otsu's binarization that helps to determine the threshold value automatically without having to sepcify one yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('starry_night.png',0)\n",
    "\n",
    "# global thresholding\n",
    "ret1,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
    "\n",
    "# otsu's thresholding (the threshold value can be chosen arbitrarily)\n",
    "ret2, th2 = cv.threshold(img, 127, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "\n",
    "# otsu's thresholding after Gaussian filtering to remove noise --> blurs the image by convolving with a Gaussian kernel\n",
    "blur = cv.GaussianBlur(img, (5,5), 0)\n",
    "ret3,th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "\n",
    "# plot all the images and their histograms\n",
    "images = [img, 0, th1,\n",
    "          img, 0, th2,\n",
    "          blur, 0, th3]\n",
    "titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',\n",
    "          'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n",
    "          'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\n",
    "for i in range(3):\n",
    "    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')\n",
    "    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)\n",
    "    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')\n",
    "    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images can be filtered with various Low Pass Filters (LPF) which help in removing noise and High Pass Filters (HPF) which help in detecting edges in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D convolution - Image Filtering\n",
    "\n",
    "img = cv.imread('opencv_logo.png')\n",
    "\n",
    "kernel = np.ones((10, 10), np.float32)/100\n",
    "dst = cv.filter2D(img, -1, kernel)\n",
    "\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Averaging')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Blurring (Image Smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Averaging\n",
    "\n",
    "# can use cv.blur() or cv.boxFilter()\n",
    "# the second argument is the kernel size used for the blurring\n",
    "blur = cv.blur(img, (5,5))\n",
    "\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(blur),plt.title('Blurred')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Gaussian Blurring --> veru effective in removing Guassian noise\n",
    "\n",
    "# third argument is sigmaX, standard deviation in X direction. You can also specify sigmaY as 4th arg\n",
    "# if sigmaY not specified, taken same as sigmaX\n",
    "blur = cv.GaussianBlur(img, (5,5), 0)\n",
    "\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(blur),plt.title('Blurred')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. medianBlur --> takes median of all pixels under kernel area and replaces central element with it\n",
    "\n",
    "median = cv.medianBlur(img, 5)\n",
    "\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(blur),plt.title('Blurred')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Bilateral Filtering --> highly effective in noise removal while keeping edges sharp\n",
    "\n",
    "# 3rd and 4th args are sigma colour and sigma space\n",
    "blur = cv.bilateralFilter(img, 9, 75, 75)\n",
    "\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Original')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(blur),plt.title('Blurred')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erosion\n",
    "It erodes away the boundaries of the foreground object. Method --> A pixel in the original image will be considered 1 only if all the pixels under the image are also 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('j.png', 0)\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "erosion = cv.erode(img, kernel, iterations = 1)\n",
    "\n",
    "cv.imshow('erosion',erosion)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dilation\n",
    "It is the oppsoite of erosion. Here a pixel element is 1 if at least one pixel under the kernel is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilation = cv.dilate(img, kernel, iterations = 1)\n",
    "\n",
    "cv.imshow('dilation', dilation)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening\n",
    "It is another name of erosion followed by dilation. Useful to remove noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opening = cv.morphologyEx(img, cv.MORPH_OPEN, kernel)\n",
    "\n",
    "cv.imshow('opening', opening)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closing\n",
    "Opposite of opening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing = cv.morphologyEx(img, cv.MORPH_CLOSE, kernel)\n",
    "\n",
    "cv.imshow('closing', closing)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Morphological Gradient\n",
    "Difference between erosion and dilation. Result looks like the outline of an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = cv.morphologyEx(img, cv.MORPH_GRADIENT, kernel)\n",
    "\n",
    "cv.imshow('gradient', gradient)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are other transformations too. You can have a look at them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient or High-Pass filters\n",
    "OpenCV provides three such filters - Sobel, Scharr and Laplacian\n",
    "\n",
    "Sobel derivatives are Gaussian smoothing plus differentiation so it is more resistant to noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing all the filters\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv.imread('gradients.jpg')\n",
    "\n",
    "laplacian = cv.Laplacian(img, cv.CV_64F)\n",
    "sobelx = cv.Sobel(img, cv.CV_64F, 1, 0, ksize = 5)\n",
    "sobely = cv.Sobel(img, cv.CV_64F, 0, 1, ksize = 5)\n",
    "\n",
    "\n",
    "plt.subplot(2,2,1),plt.imshow(img,cmap = 'gray')\n",
    "plt.title('Original'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,2,2),plt.imshow(laplacian,cmap = 'gray')\n",
    "plt.title('Laplacian'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,2,3),plt.imshow(sobelx,cmap = 'gray')\n",
    "plt.title('Sobel X'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(2,2,4),plt.imshow(sobely,cmap = 'gray')\n",
    "plt.title('Sobel Y'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most famous algorithm used is the Canny detection algorithm which has multiple steps:  \n",
    "1) Noise Reduction  \n",
    "2) Finding intensity gradient of the image  \n",
    "3) Non-maximum supression  \n",
    "4) Hysterisis Thresholding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('j.png')\n",
    "edges = cv.Canny(img, 100, 200)\n",
    "\n",
    "plt.subplot(121),plt.imshow(img,cmap = 'gray')\n",
    "plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(122),plt.imshow(edges,cmap = 'gray')\n",
    "plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, cv.Canny() wraps all the steps in that one function. 2nd and 3rd args are the minVal and maxVal used for hysterisis thresholding. 4th arg is the aperture_size which is the size of the Sobel kernel used to find the image gradients (in the x and y directions). Last arg is the L2 gradient for finding gradient magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contours\n",
    "Contours can be described as a curve joining all continuous points. For better accuracy, use binary images, hence apply threshold or canny edge detection\n",
    "\n",
    "The approximation cv.CHAIN_APPROX_NONE saves all the contour boundary points while the approximation function cv.CHAIN_APPROX_SIMPLE removes redundant points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('j.png')\n",
    "imgray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv.threshold(imgray, 127, 255, 0)\n",
    "# 2nd argument is the contour retrieval mode and 3rd is the contour approximation method\n",
    "img, contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth arg onwards are colour, thickness etc\n",
    "# to draw all the contours in the image, use -1\n",
    "cv.drawContours(img, contours, -1, (0,255,0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also use the following way to draw a specific contour\n",
    "cnt = contours[4]\n",
    "cv.drawContours(img, [cnt], 0, (0,255,0), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moments\n",
    "These help you to calculate important parameters like centre of mass, area, perimeter etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = contours[0]\n",
    "M = cv.moments(cnt)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid\n",
    "cx = int(M['m10']/M['m00'])\n",
    "cy = int(M['m01']/M['m00'])\n",
    "\n",
    "# area\n",
    "area = cv.contourArea(cnt)\n",
    "\n",
    "# contour perimeter\n",
    "perimeter = cv.arcLength(cnt, True) # 2nd parameter specifies whether shape is a closed contour (if passed True) or just a curve\n",
    "\n",
    "# contour approximation --> it approximates a contour shape to another shape with less number of vertices based on precision\n",
    "# uses Douglas - Peucker algorithm\n",
    "# Eg. trying to find a square but due to some problems in the image we don't get a perfect square\n",
    "# epsilon is an accuracy parameter and specifies maximum distance from contour to approximated contour\n",
    "epsilon = 0.1*cv.arcLength(cnt, True)\n",
    "approx = cv.approxPolyDB(cnt, epsilon, True) # 3rd arg tells whether curve is closed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convex hull --> it checks for convexity defects and corrects it\n",
    "# syntax: hull = cv.convexHull(points[, hull[, clockwise[, returnPoints]]\n",
    "# the arg 'returnPoints' if true, returns coordinates of hull points. If false, it returns indices of contour points corresponding to hull points\n",
    "hull = cv.convexHull(cnt)\n",
    "\n",
    "# to check whether a curve is convex (curves which are always bulged out) or not\n",
    "hull= cv.isContourConvex(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounding a rectangle\n",
    "\n",
    "cnt = contours[0]\n",
    "\n",
    "# straight bounding rectangle does not consider rotation of object, so is not rectangle of min area\n",
    "x, y, w, h = cv.boundingRect(cnt) # x,y => top-left coord of rect and w,h => width and height of rect\n",
    "cv.rectangle(img, (x,y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "# rotated rectangle is the bounding rectangle of min area and takes rotation into account\n",
    "# to draw this rectangle, we need the four points using the function cv.boxPoints()\n",
    "rect = cv.minAreaRect(cnt)\n",
    "box = cv.boxPoints(rect)\n",
    "box = np.int0(box)\n",
    "cv.drawContours(img, [box], 0, (0, 0, 255), 2)\n",
    "\n",
    "# min enclosing circle\n",
    "(x,y), radius = cv.minEnclosingCircle(cnt)\n",
    "center = (int(x), int(y))\n",
    "radius = int(radius)\n",
    "cv.circle(img, center, radius, (0, 255, 0), 2)\n",
    "\n",
    "# fitting an ellipse --> returns ellipse that inscribes the min bounding rect\n",
    "ellipse = cv.fitEllipse(cnt)\n",
    "cv.ellipse(img, ellipse, (0, 255, 0), 2)\n",
    "\n",
    "# fitting a line --> approximate a straight line\n",
    "rows, cols = img.shape[:2]\n",
    "[vx,vy,x,y] = cv.fitLine(cnt, cv.DIST_L2,0,0.01,0.01)\n",
    "lefty = int((-x*vy/vx) + y)\n",
    "righty = int(((cols-x)*vy/vx)+y)\n",
    "cv.line(img,(cols-1,righty),(0,lefty),(0,255,0),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other contour properties:  \n",
    "\n",
    "1) Aspect Ratio: ratio of width to height of bounding rectangle  \n",
    "2) Extent: ratio of contour area to bounding rectangle area  \n",
    "3) Solidity: ratio of contour area to convex hull area  \n",
    "4) Equivalent diameter: diameter of the circle whose area is equal to contour area  \n",
    "5) Orientation: angle at which object is directed  \n",
    "6) Extreme Points: found as follows-  \n",
    "\n",
    "leftmost = tuple(cnt[cnt[:, :, 0].argmin()][0])  \n",
    "rightmost = tuple(cnt[cnt[:, :, 0].argmax()][0])  \n",
    "topmost = tuple(cnt[cnt[:, :, 1].argmin()][0])  \n",
    "bottommost = tuple(cnt[cnt[:, :, 1].argmax()][0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding convexity defects and drawing them\n",
    "# it returns an array where each row contains [start_point, end_point, farthest_point, approximate distance to farthest_point]\n",
    "defects = cv.convexityDefects(cnt, hull) # need to pass returnPoints = False while finding convexity defects\n",
    "\n",
    "for i in range(defects.shape[0]):\n",
    "    s,e,f,d = defects[i,0]\n",
    "    start = tuple(cnt[s][0])\n",
    "    end = tuple(cnt[e][0])\n",
    "    far = tuple(cnt[f][0])\n",
    "    cv.line(img,start,end,[0,255,0],2)\n",
    "    cv.circle(img,far,5,[0,0,255],-1)\n",
    "    \n",
    "cv.imshow('img',img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point polygon test --> finds shortest distance between point on the image and the contour\n",
    "# returns positive if point inside contour otherwise returns negative\n",
    "dist = cv.pointPolygonTest(cnt, (50, 50), True) # last arg finds signed distance if True, else finds whether point inside or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match shapes --> compare two shapes/contours and return similarity metric. Lower the metric, more the similarity\n",
    "# calculated based on hu-moment values\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "img1 = cv.imread('star.jpg',0)\n",
    "img2 = cv.imread('star2.jpg',0)\n",
    "ret, thresh = cv.threshold(img1, 127, 255,0)\n",
    "ret, thresh2 = cv.threshold(img2, 127, 255,0)\n",
    "contours,hierarchy = cv.findContours(thresh,2,1)\n",
    "cnt1 = contours[0]\n",
    "contours,hierarchy = cv.findContours(thresh2,2,1)\n",
    "cnt2 = contours[0]\n",
    "\n",
    "ret = cv.matchShapes(cnt1,cnt2,1,0.0)\n",
    "print( ret )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contour hierarchy in OpenCV:  \n",
    "In some cases, some shapes can be inside other shapes, just like nested figure. In this case, we call the outer shape as the parent and the inner contour as the child.  \n",
    "\n",
    "Hierarchy Representation in OpenCV: [Next, Previous, First_Child, Parent]  \n",
    "Next is the next contour in same hierarchy level  \n",
    "Previous is the previous contour in same hierarchy level  \n",
    "First_Child is the first child of the contour  \n",
    "Parent is the parent of the contour  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can consider histogram as a graph or plot, which gives you an overall idea about the intensity distribution of an image. It is a plot with pixel values (ranging from 0 to 255, not always) in X-axis and corresponding number of pixels in the image on Y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('starry_night.png')\n",
    "# 2nd arg is channel for which you want histogram. For colored image, it can be [0], [1] or [2]\n",
    "# 3rd arg is histSize, that is, number of bins\n",
    "# 4th arg is the range, that is, range of intensity values you want to measure. Normally it is is [0, 256]\n",
    "hist = cv.calcHist([img], [0], None, [256], [0, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the histogram\n",
    "\n",
    "# matplotlib way\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(img.ravel(), 256, [0, 256])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following can be a good way for BGR plots:\n",
    "color = ('b','g','r')\n",
    "for i,col in enumerate(color):\n",
    "    histr = cv.calcHist([img],[i],None,[256],[0,256])\n",
    "    plt.plot(histr,color = col)\n",
    "    plt.xlim([0,256])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is also an OpenCV way to plot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application of mask --> for only a portion of image\n",
    "\n",
    "# create a mask\n",
    "mask = np.zeros(img.shape[:2], np.uint8)\n",
    "mask[100:300, 100:400] = 255\n",
    "masked_img = cv.bitwise_and(img,img,mask = mask)\n",
    "# Calculate histogram with mask and without mask\n",
    "# Check third argument for mask\n",
    "hist_full = cv.calcHist([img],[0],None,[256],[0,256])\n",
    "hist_mask = cv.calcHist([img],[0],mask,[256],[0,256])\n",
    "plt.subplot(221), plt.imshow(img, 'gray')\n",
    "plt.subplot(222), plt.imshow(mask,'gray')\n",
    "plt.subplot(223), plt.imshow(masked_img, 'gray')\n",
    "plt.subplot(224), plt.plot(hist_full), plt.plot(hist_mask)\n",
    "plt.xlim([0,256])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram Equalization\n",
    "\n",
    "It improves the contrast of the image by stretching the histogram on either ends if it is confined to specific values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('face.jpg', 0)\n",
    "equ = cv.equalizeHist(img)\n",
    "res = np.hstack((img, equ)) # stacking images side by side\n",
    "cv.imwrite('res.png', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram equalization is good in cases where all images with different lighting conditions are to be brought to a common scale. Eg. for facial recognition before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "\n",
    "This is important in scenarios where some information might be lost due to over-brightness after applying histogram equalization. CLAHE uses tiles and equalizes histograms of smaller regions at a time. If any histogram bin is above the specifies contrast limit, those pixels are clipped and distributed uniformly to other bins before applying histogram equalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a clahe object\n",
    "clahe = cv.createCLAHE(clipLimit = 2.0, tileGridSize = (8,8))\n",
    "cl1 = clahe.apply(img)\n",
    "\n",
    "cv.imwrite('res.png', cl1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D histogram\n",
    "Earlier we were dealing with 1D histograms as they had only one feature: intensity for every pixel. In 2D histograms, we use two features, namely hue and saturation for every pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('starry_night.png')\n",
    "hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "\n",
    "# 180 bins for hue and 256 bins for saturation\n",
    "hist = cv.calcHist([hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib function to display 2D histogram\n",
    "plt.imshow(hist,interpolation = 'nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram Backprojection\n",
    "In simple words, it creates an image of the same size (but single channel) as that of our input image, where each pixel corresponds to the probability of that pixel belonging to our object. In more simpler words, the output image will have our object of interest in more white compared to remaining part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = cv.imread('starry_night.png')\n",
    "hsv = cv.cvtColor(roi, cv.COLOR_BGR2HSV)\n",
    "\n",
    "target = cv.imread('opencv_logo.png')\n",
    "hsvt = cv.cvtColor(target, cv.COLOR_BGR2HSV)\n",
    "\n",
    "# calculating object histogram\n",
    "roihist = cv.calcHist([hsv], [0,1], None, [180,256], [0,180,0,256])\n",
    "\n",
    "# normalize histogram and apply backprojection\n",
    "cv.normalize(roihist, 0, 255, cv.NORM_MINMAX)\n",
    "dst = cv.calcBackProject([hsvt], [0,1], roihist, [0, 180, 0, 256], 1) # this gives us the probability image\n",
    "\n",
    "# now convolve with circular disc with the probability image obtained above\n",
    "disc = cv.getStructuringElement(cv.MORPH_ELLIPSE, (5,5))\n",
    "cv.filter2D(dst, -1, disc, dst)\n",
    "\n",
    "# threshold anf binary AND\n",
    "ret, thresh = cv.threshold(dst, 50, 255, 0)\n",
    "thresh = cv.merge((thresh, thresh, thresh))\n",
    "res = cv.bitwise_and(target, thresh)\n",
    "\n",
    "res = np.vstack((target, thresh, res))\n",
    "cv.imwrite('res.png', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template Matching\n",
    "It is used for searching and finding the location of a template image in a larger image. It simply slides the template image over the larger image and helps find the best match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code\n",
    "\n",
    "img = cv.imread('messi5.jpg',0)\n",
    "img2 = img.copy()\n",
    "template = cv.imread('template.jpg',0)\n",
    "w, h = template.shape[::-1]\n",
    "# All the 6 methods for comparison in a list\n",
    "methods = ['cv.TM_CCOEFF', 'cv.TM_CCOEFF_NORMED', 'cv.TM_CCORR',\n",
    "            'cv.TM_CCORR_NORMED', 'cv.TM_SQDIFF', 'cv.TM_SQDIFF_NORMED']\n",
    "for meth in methods:\n",
    "    img = img2.copy()\n",
    "    method = eval(meth)\n",
    "    # Apply template Matching\n",
    "    res = cv.matchTemplate(img,template,method)\n",
    "    min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)\n",
    "    # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum\n",
    "    if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:\n",
    "        top_left = min_loc\n",
    "    else:\n",
    "        top_left = max_loc\n",
    "    bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "    cv.rectangle(img,top_left, bottom_right, 255, 2)\n",
    "    plt.subplot(121),plt.imshow(res,cmap = 'gray')\n",
    "    plt.title('Matching Result'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(122),plt.imshow(img,cmap = 'gray')\n",
    "    plt.title('Detected Point'), plt.xticks([]), plt.yticks([])\n",
    "    plt.suptitle(meth)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template matching with multiple objects uses thresholding\n",
    "\n",
    "img_rgb = cv.imread('mario.png')\n",
    "img_gray = cv.cvtColor(img_rgb, cv.COLOR_BGR2GRAY)\n",
    "template = cv.imread('mario_coin.png',0)\n",
    "w, h = template.shape[::-1]\n",
    "res = cv.matchTemplate(img_gray,template,cv.TM_CCOEFF_NORMED)\n",
    "threshold = 0.8\n",
    "loc = np.where( res >= threshold)\n",
    "for pt in zip(*loc[::-1]):\n",
    "    cv.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2)\n",
    "cv.imwrite('res.png',img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Detection using Hough Transform\n",
    "\n",
    "Need to apply thresholding and canny edge detection for the grayscale image, to apply hough transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('sudoku.png')\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "edges = cv.Canny(gray, 50, 150, apertureSize = 3)\n",
    "\n",
    "lines = cv.HoughLines(edges, 1, np.pi/180, 200)\n",
    "for line in lines:\n",
    "    # rho is perpendicular distance of line from origin and theta is angle from the origin\n",
    "    rho, theta = line[0]\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    x0 = a*rho\n",
    "    y0 = b*rho\n",
    "    x1 = int(x0 + 1000*(-b))\n",
    "    y1 = int(y0 + 1000*(a))\n",
    "    x2 = int(x0 - 1000*(-b))\n",
    "    y2 = int(y0 - 1000*(a))\n",
    "    \n",
    "    cv.line(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "\n",
    "cv.imwrite('houghlines3.jpg',img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Hough Transform\n",
    "It takes a random subset of points instead of all points for the hough computation. Makes it less expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minLineLength: minimum length of the line. Lines shorter than this are rejected\n",
    "# maxLineGap: maximum allowed gap between line segments to treat them as a single line\n",
    "lines = cv.HoughLinesP(edges,1,np.pi/180,100,minLineLength=100,maxLineGap=10)\n",
    "for line in lines:\n",
    "    x1,y1,x2,y2 = line[0]\n",
    "    cv.line(img,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "cv.imwrite('houghlines5.jpg',img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hough Circle Transform\n",
    "\n",
    "Parameters\n",
    "image\t8-bit, single-channel, grayscale input image.  \n",
    "\n",
    "circles\tOutput vector of found circles. Each vector is encoded as 3 or 4 element floating-point vector (x,y,radius) or (x,y,radius,votes) .  \n",
    "\n",
    "method\tDetection method, see HoughModes. The available methods are HOUGH_GRADIENT and HOUGH_GRADIENT_ALT.  \n",
    "\n",
    "dp\tInverse ratio of the accumulator resolution to the image resolution. For example, if dp=1 , the accumulator has the same \n",
    "resolution as the input image. If dp=2 , the accumulator has half as big width and height. For HOUGH_GRADIENT_ALT the recommended value is dp=1.5, unless some small very circles need to be detected.  \n",
    "\n",
    "minDist\tMinimum distance between the centers of the detected circles. If the parameter is too small, multiple neighbor circles may be falsely detected in addition to a true one. If it is too large, some circles may be missed.  \n",
    "\n",
    "param1\tFirst method-specific parameter. In case of HOUGH_GRADIENT and HOUGH_GRADIENT_ALT, it is the higher threshold of the two passed to the Canny edge detector (the lower one is twice smaller). Note that HOUGH_GRADIENT_ALT uses Scharr algorithm to compute image derivatives, so the threshold value shough normally be higher, such as 300 or normally exposed and contrasty images.  \n",
    "\n",
    "param2\tSecond method-specific parameter. In case of HOUGH_GRADIENT, it is the accumulator threshold for the circle centers at the detection stage. The smaller it is, the more false circles may be detected. Circles, corresponding to the larger accumulator values, will be returned first. In the case of HOUGH_GRADIENT_ALT algorithm, this is the circle \"perfectness\" measure. The closer it to 1, the better shaped circles algorithm selects. In most cases 0.9 should be fine. If you want get better detection of small circles, you may decrease it to 0.85, 0.8 or even less. But then also try to limit the search range [minRadius, maxRadius] to avoid many false circles.  \n",
    "\n",
    "minRadius\tMinimum circle radius.  \n",
    "\n",
    "maxRadius\tMaximum circle radius. If <= 0, uses the maximum image dimension. If < 0, HOUGH_GRADIENT returns centers without finding the radius. HOUGH_GRADIENT_ALT always computes circle radiuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('opencv_logo.png',0)\n",
    "img = cv.medianBlur(img,5)\n",
    "cimg = cv.cvtColor(img,cv.COLOR_GRAY2BGR)\n",
    "circles = cv.HoughCircles(img,cv.HOUGH_GRADIENT,1,100,\n",
    "                            param1=50,param2=30,minRadius=0,maxRadius=0)\n",
    "circles = np.uint16(np.around(circles))\n",
    "for i in circles[0,:]:\n",
    "    # draw the outer circle\n",
    "    cv.circle(cimg,(i[0],i[1]),i[2],(0,255,0),2)\n",
    "    # draw the center of the circle\n",
    "    cv.circle(cimg,(i[0],i[1]),2,(0,0,255),3)\n",
    "cv.imshow('detected circles',cimg)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watershed Algorithm for image segmentation\n",
    "\n",
    "Label the region which we are sure of being the foreground or object with one color (or intensity), label the region which we are sure of being background or non-object with another color and finally the region which we are not sure of anything, label it with 0. That is our marker. Then apply watershed algorithm. Then our marker will be updated with the labels we gave, and the boundaries of objects will have a value of -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('coins.jpg')\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv.threshold(gray, 0, 255, cv.THRESH_BINARY_INV + cv.THRESH_OTSU)\n",
    "\n",
    "plt.imshow(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise removal\n",
    "kernel = np.ones((3,3), np.uint8)\n",
    "opening = cv.morphologyEx(thresh, cv.MORPH_OPEN, kernel, iterations = 2) # erosion followed by dilation\n",
    "\n",
    "# sure background area\n",
    "sure_bg = cv.dilate(opening, kernel, iterations = 3)\n",
    "\n",
    "# finding sure foreground area\n",
    "dist_transform = cv.distanceTransform(opening, cv.DIST_L2, 5)\n",
    "ret, sure_fg = cv.threshold(dist_transform, 0.7*dist_transform.max(), 255, 0)\n",
    "\n",
    "# finding unknown region\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "unknown = cv.subtract(sure_bg, sure_fg)\n",
    "\n",
    "plt.imshow(dist_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marker labelling\n",
    "# cv.connectedComponents() labels background as 0 and other objects with integers starting from 1\n",
    "# however, we want unknown area to marked as 0, as required by watershed algorithm\n",
    "ret, markers = cv.connectedComponents(sure_fg)\n",
    "\n",
    "# add one to all the markers so that sure_bg is not 0, but 1\n",
    "markers += 1\n",
    "\n",
    "# now mark the region of unknown with 0\n",
    "markers[unknown == 255] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now apply watershed\n",
    "# the marker image will be modified and the boundary region will be marked with -1\n",
    "markers = cv.watershed(img, markers)\n",
    "img[markers == -1] = [255, 0, 0]\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive foreground extraction using Grabcut Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv.imread('messi.jpg')\n",
    "mask = np.zeros(img.shape[:2], np.uint8)\n",
    "\n",
    "# used by the algorthm internally\n",
    "bgdModel = np.zeros((1, 65), np.float64)\n",
    "fgdModel = np.zeros((1, 65), np.float64)\n",
    "\n",
    "rect = (50, 50, 450, 290)\n",
    "cv.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv.GC_INIT_WITH_RECT)\n",
    "\n",
    "# all 0 and 2 pixels are background\n",
    "mask2 = np.where((mask == 2)|(mask == 0), 0, 1).astype('uint8')\n",
    "img = img*mask2[:,:,np.newaxis]\n",
    "\n",
    "plt.imshow(img), plt.colorbar(), plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can improve the above image by marking sure foreground with white and sure background with black in the image on Paint, and then re-run the algorithm with this edited image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Analysis (Video Module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Subtraction\n",
    "\n",
    "Background modelling consists of:  \n",
    "Background Initialization - initial model of background  \n",
    "Background Update - To possible changes to the background  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cv2 as cv\n",
    "\n",
    "backSub = cv.createBackgroundSubtractorMOG2()\n",
    "\n",
    "\n",
    "capture = cv.VideoCapture(0)\n",
    "if not capture.isOpened:\n",
    "    print('Unable to open')\n",
    "    exit(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "    fgMask = backSub.apply(frame)\n",
    "    cv.rectangle(frame, (10, 2), (100,20), (255,255,255), -1)\n",
    "    cv.putText(frame, str(capture.get(cv.CAP_PROP_POS_FRAMES)), (15, 15),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,0))\n",
    "    \n",
    "    \n",
    "    cv.imshow('Frame', frame)\n",
    "    cv.imshow('FG Mask', fgMask)\n",
    "    \n",
    "    keyboard = cv.waitKey(30)\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meanshift\n",
    "An algorithm to track objects in videos by continuously moving a window to area with highest pixel density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "# take first frame of video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# setup initial location of window\n",
    "x, y, w, h = 300, 200, 100, 50 # simply hardcoded the values\n",
    "track_window = (x, y, w, h)\n",
    "\n",
    "# set up the ROI for tracking\n",
    "roi = frame[y:y+h, x:x+w]\n",
    "hsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\n",
    "mask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\n",
    "roi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\n",
    "cv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\n",
    "\n",
    "# Setup the termination criteria, either 10 iteration or move by atleast 1 pt\n",
    "term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret == True:\n",
    "        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "        dst = cv.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
    "        \n",
    "        # apply meanshift to get the new location\n",
    "        ret, track_window = cv.meanShift(dst, track_window, term_crit)\n",
    "        \n",
    "        # Draw it on image\n",
    "        x,y,w,h = track_window\n",
    "        img2 = cv.rectangle(frame, (x,y), (x+w,y+h), 255,2)\n",
    "        cv.imshow('img2',img2)\n",
    "        \n",
    "        k = cv.waitKey(30) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cv.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camshift (Continously Adaptive Mean Shift)\n",
    "It updates the size of the window by applying meanshift first and then updating size of window through a formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "# take first frame of the video\n",
    "ret,frame = cap.read()\n",
    "# setup initial location of window\n",
    "x, y, w, h = 300, 200, 100, 50 # simply hardcoded the values\n",
    "track_window = (x, y, w, h)\n",
    "# set up the ROI for tracking\n",
    "roi = frame[y:y+h, x:x+w]\n",
    "hsv_roi =  cv.cvtColor(roi, cv.COLOR_BGR2HSV)\n",
    "mask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.)))\n",
    "roi_hist = cv.calcHist([hsv_roi],[0],mask,[180],[0,180])\n",
    "cv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX)\n",
    "# Setup the termination criteria, either 10 iteration or move by atleast 1 pt\n",
    "term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "        dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "        # apply camshift to get the new location\n",
    "        ret, track_window = cv.CamShift(dst, track_window, term_crit)\n",
    "        # Draw it on image\n",
    "        pts = cv.boxPoints(ret)\n",
    "        pts = np.int0(pts)\n",
    "        img2 = cv.polylines(frame,[pts],True, 255,2)\n",
    "        cv.flip(img2, flipCode = -1)\n",
    "        cv.imshow('img2',img2)\n",
    "        k = cv.waitKey(30) & 0xff\n",
    "        if k == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cv.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optical Flow\n",
    "It is the pattern of apparent motion of objects between two consecutive frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lucas-Kanade Optical Flow\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "# params for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "# Create some random colors\n",
    "color = np.random.randint(0,255,(100,3))\n",
    "# Take first frame and find corners in it\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)\n",
    "p0 = cv.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "while(1):\n",
    "    ret,frame = cap.read()\n",
    "    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    # calculate optical flow\n",
    "    p1, st, err = cv.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "    # Select good points\n",
    "    good_new = p1[st==1]\n",
    "    good_old = p0[st==1]\n",
    "    # draw the tracks\n",
    "    for i,(new,old) in enumerate(zip(good_new, good_old)):\n",
    "        a,b = new.ravel()\n",
    "        c,d = old.ravel()\n",
    "        mask = cv.line(mask, (a,b),(c,d), color[i].tolist(), 2)\n",
    "        frame = cv.circle(frame,(a,b),5,color[i].tolist(),-1)\n",
    "    img = cv.add(frame,mask)\n",
    "    cv.flip(img, flipCode = -1)\n",
    "    cv.imshow('frame',img)\n",
    "    k = cv.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    # Now update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1,1,2)\n",
    "cv.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Optical Flow using Gunner Farneback's algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "cap = cv.VideoCapture(0)\n",
    "ret, frame1 = cap.read()\n",
    "prvs = cv.cvtColor(frame1,cv.COLOR_BGR2GRAY)\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[...,1] = 255\n",
    "while(1):\n",
    "    ret, frame2 = cap.read()\n",
    "    next = cv.cvtColor(frame2,cv.COLOR_BGR2GRAY)\n",
    "    flow = cv.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    mag, ang = cv.cartToPolar(flow[...,0], flow[...,1])\n",
    "    hsv[...,0] = ang*180/np.pi/2\n",
    "    hsv[...,2] = cv.normalize(mag,None,0,255,cv.NORM_MINMAX)\n",
    "    bgr = cv.cvtColor(hsv,cv.COLOR_HSV2BGR)\n",
    "    cv.imshow('frame2',bgr)\n",
    "    k = cv.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    elif k == ord('s'):\n",
    "        cv.imwrite('opticalfb.png',frame2)\n",
    "        cv.imwrite('opticalhsv.png',bgr)\n",
    "    prvs = next\n",
    "cv.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth map from stereo images\n",
    "A disparity map refers to the apparent pixel difference or motion between two stereo images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "imgL = cv.imread('tsukuba_l.png',0)\n",
    "imgR = cv.imread('tsukuba_r.png',0)\n",
    "stereo = cv.StereoBM_create(numDisparities=16, blockSize=15)\n",
    "disparity = stereo.compute(imgL,imgR)\n",
    "plt.imshow(disparity,'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
